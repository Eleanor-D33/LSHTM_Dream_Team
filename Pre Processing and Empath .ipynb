{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used for pre-processing \n",
    "\n",
    "- https://towardsdatascience.com/an-easy-tutorial-about-sentiment-analysis-with-deep-learning-and-keras-2bf52b9cba91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove URLs from the tweets\n",
    "* Tokenize text\n",
    "* Remove emails\n",
    "* Remove new lines characters\n",
    "* Remove distracting single quotes\n",
    "* Remove all punctuation signs\n",
    "* Lowercase all text\n",
    "* Detokenize text\n",
    "* Convert list of texts to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# pip -- install nltk \n",
    "\n",
    "# Importing relevant libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tw_date</th>\n",
       "      <th>place_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>name</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>coord_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200923972742848512</td>\n",
       "      <td>Huge thank you to @Empowerment_ #peer4U projec...</td>\n",
       "      <td>2</td>\n",
       "      <td>peer4U,codesign</td>\n",
       "      <td>2019-11-30T23:46:01.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>13b1318251fcfc7c</td>\n",
       "      <td>Bilsborrow, England</td>\n",
       "      <td>Bilsborrow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200922864142241793</td>\n",
       "      <td>Woohoo! First new friend since moving to Liver...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:41:37.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>151b9e91272233d1</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200918723449606146</td>\n",
       "      <td>I had anxiety about conceiving for a couple of...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:25:10.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>38eabeb176f29378</td>\n",
       "      <td>East Kilbride, Scotland</td>\n",
       "      <td>East Kilbride</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1200914479661359104</td>\n",
       "      <td>@JBattye I wouldn’t even know where to look! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:08:18.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>778909dfad43f3d6</td>\n",
       "      <td>Huddersfield, England</td>\n",
       "      <td>Huddersfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1200914017679814661</td>\n",
       "      <td>Bloody hell Asda @AsdaServiceTeam shopping due...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:06:28.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>6da127116b06cece</td>\n",
       "      <td>Upchurch, England</td>\n",
       "      <td>Upchurch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1200923972742848512  Huge thank you to @Empowerment_ #peer4U projec...   \n",
       "1  1200922864142241793  Woohoo! First new friend since moving to Liver...   \n",
       "2  1200918723449606146  I had anxiety about conceiving for a couple of...   \n",
       "3  1200914479661359104  @JBattye I wouldn’t even know where to look! I...   \n",
       "4  1200914017679814661  Bloody hell Asda @AsdaServiceTeam shopping due...   \n",
       "\n",
       "   hashtags_cnt         hashtags                created_at     tw_date  \\\n",
       "0             2  peer4U,codesign  2019-11-30T23:46:01.000Z  2019-11-30   \n",
       "1             0              NaN  2019-11-30T23:41:37.000Z  2019-11-30   \n",
       "2             0              NaN  2019-11-30T23:25:10.000Z  2019-11-30   \n",
       "3             0              NaN  2019-11-30T23:08:18.000Z  2019-11-30   \n",
       "4             0              NaN  2019-11-30T23:06:28.000Z  2019-11-30   \n",
       "\n",
       "           place_id                full_name           name coordinates  \\\n",
       "0  13b1318251fcfc7c      Bilsborrow, England     Bilsborrow         NaN   \n",
       "1  151b9e91272233d1       Liverpool, England      Liverpool         NaN   \n",
       "2  38eabeb176f29378  East Kilbride, Scotland  East Kilbride         NaN   \n",
       "3  778909dfad43f3d6    Huddersfield, England   Huddersfield         NaN   \n",
       "4  6da127116b06cece        Upchurch, England       Upchurch         NaN   \n",
       "\n",
       "  coord_type  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data \n",
    "\n",
    "tweet_text = pd.read_csv('/Users/eleanordavies/Desktop/Twiiter_data5.csv')\n",
    "tweet_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to remove characters \n",
    "\n",
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huge thank you to #peer4U project for allowing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Woohoo! First new friend since moving to Liver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had anxiety about conceiving for a couple of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wouldn’t even know where to look! IMO there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bloody hell Asda shopping due between 8-9pm. I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Huge thank you to #peer4U project for allowing...\n",
       "1  Woohoo! First new friend since moving to Liver...\n",
       "2  I had anxiety about conceiving for a couple of...\n",
       "3  I wouldn’t even know where to look! IMO there ...\n",
       "4  Bloody hell Asda shopping due between 8-9pm. I..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "#Splitting twtter data text column to list\n",
    "data_to_list = tweet_text['text'].values.tolist()\n",
    "\n",
    "#applying the above function \n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "\n",
    "# tempdf = pd.DataFrame(temp)\n",
    "# tempdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['huge', 'thank', 'you', 'to', 'peer', 'project', 'for', 'allowing', 'me', 'to', 'design', 'my', 'ma', 'research', 'exploring', 'the', 'impact', 'of', 'peer', 'mentor', 'model', 'on', 'reducing', 'loneliness', 'thank', 'you', 'and', 'the', 'team', 'of', 'mentors', 'and', 'mentees', 'codesign'], ['woohoo', 'first', 'new', 'friend', 'since', 'moving', 'to', 'liverpool', 'and', 'its', 'only', 'been', 'few', 'weeks', 'for', 'an', 'introvert', 'with', 'social', 'anxiety', 'im', 'really', 'getting', 'better', 'at', 'this'], ['had', 'anxiety', 'about', 'conceiving', 'for', 'couple', 'of', 'reasons', 'and', 'then', 'this', 'wee', 'miracle', 'enter', 'our', 'lives', 'for', 'me', 'this', 'is', 'the', 'best', 'xmas', 'gift', 'and', 'cant', 'even', 'describe', 'how', 'privileged', 'feel', 'to', 'be', 'her', 'mum', 'excuse', 'the', 'messy', 'paint', 'work', 'and', 'feel', 'free', 'to', 'admire', 'the', 'buffy', 'funko', 'pop'], ['wouldn', 'even', 'know', 'where', 'to', 'look', 'imo', 'there', 'is', 'far', 'too', 'much', 'on', 'the', 'walls', 'not', 'teacher', 'so', 'can', 'give', 'professional', 'opinion', 'but', 'if', 'that', 'amount', 'of', 'busy', 'would', 'stress', 'me', 'out', 'what', 'about', 'the', 'kids'], ['bloody', 'hell', 'asda', 'shopping', 'due', 'between', 'pm', 'it', 'now', 'pm', 'and', 'still', 'no', 'shopping', 'don', 'think', 'you', 'realise', 'how', 'much', 'anxiety', 'this', 'is', 'causing', 'right', 'now'], ['no', 'stress'], ['anxiety', 'plz', 'get', 'in', 'the', 'bin'], ['im', 'already', 'sick', 'and', 'peoples', 'are', 'trying', 'to', 'give', 'me', 'stress'], ['ive', 'written', 'some', 'new', 'tunes', 'during', 'my', 'anxiety', 'ridden', 'last', 'weeks', 'im', 'happy', 'with', 'the', 'music', 'but', 'my', 'vocals', 'bit', 'shit', 'needs', 'work', 'or', 'vocoder'], ['anxiety', 'fucking', 'sucks']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and removal all punctation, emojis and puts text into lowercase \n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "data_words = list(sent_to_words(temp))\n",
    "\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenise returns them back into a sentence \n",
    "\n",
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huge thank you to peer project for allowing me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woohoo first new friend since moving to liverp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>had anxiety about conceiving for couple of rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wouldn even know where to look imo there is fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bloody hell asda shopping due between pm it no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49025</th>\n",
       "      <td>well weird watching bits of virtual fashion sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49026</th>\n",
       "      <td>depression mentalhealth addiction recovery anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49027</th>\n",
       "      <td>tried to adapt worksheet to allow pupils to di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49028</th>\n",
       "      <td>waltz away dreaming would be my favourite tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49029</th>\n",
       "      <td>how virtual reality is helping patients with p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49030 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      huge thank you to peer project for allowing me...\n",
       "1      woohoo first new friend since moving to liverp...\n",
       "2      had anxiety about conceiving for couple of rea...\n",
       "3      wouldn even know where to look imo there is fa...\n",
       "4      bloody hell asda shopping due between pm it no...\n",
       "...                                                  ...\n",
       "49025  well weird watching bits of virtual fashion sh...\n",
       "49026  depression mentalhealth addiction recovery anx...\n",
       "49027  tried to adapt worksheet to allow pupils to di...\n",
       "49028     waltz away dreaming would be my favourite tiff\n",
       "49029  how virtual reality is helping patients with p...\n",
       "\n",
       "[49030 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_detoken = []\n",
    "for i in range(len(data_words)):\n",
    "    data_detoken.append(detokenize(data_words[i]))\n",
    "data_detoken = np.array(data_detoken)\n",
    "data_detoken_df = pd.DataFrame(data_detoken)\n",
    "data_detoken_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_detoken_df.to_csv('/Users/eleanordavies/Desktop/df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cleaned tweet text as column 'selected_tweets' to original dataframe \n",
    "frames = [data_detoken_df,tweet_text]\n",
    "demo_tweets_clean = pd.concat(frames, axis=1)\n",
    "\n",
    "demo_tweets_clean = demo_tweets_clean.rename(columns={0: \"selected_tweets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets Clean!\n"
     ]
    }
   ],
   "source": [
    "# Exporting the cleaned tweets to local directory \n",
    "\n",
    "# demo_tweets_clean.to_csv('/Users/eleanordavies/Desktop/demo_tweets_clean.csv', index = False)\n",
    "print(\"Tweets Clean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_tweets</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tw_date</th>\n",
       "      <th>place_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>name</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>coord_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huge thank you to peer project for allowing me...</td>\n",
       "      <td>1200923972742848512</td>\n",
       "      <td>Huge thank you to @Empowerment_ #peer4U projec...</td>\n",
       "      <td>2</td>\n",
       "      <td>peer4U,codesign</td>\n",
       "      <td>2019-11-30T23:46:01.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>13b1318251fcfc7c</td>\n",
       "      <td>Bilsborrow, England</td>\n",
       "      <td>Bilsborrow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woohoo first new friend since moving to liverp...</td>\n",
       "      <td>1200922864142241793</td>\n",
       "      <td>Woohoo! First new friend since moving to Liver...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:41:37.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>151b9e91272233d1</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>had anxiety about conceiving for couple of rea...</td>\n",
       "      <td>1200918723449606146</td>\n",
       "      <td>I had anxiety about conceiving for a couple of...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:25:10.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>38eabeb176f29378</td>\n",
       "      <td>East Kilbride, Scotland</td>\n",
       "      <td>East Kilbride</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wouldn even know where to look imo there is fa...</td>\n",
       "      <td>1200914479661359104</td>\n",
       "      <td>@JBattye I wouldn’t even know where to look! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:08:18.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>778909dfad43f3d6</td>\n",
       "      <td>Huddersfield, England</td>\n",
       "      <td>Huddersfield</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bloody hell asda shopping due between pm it no...</td>\n",
       "      <td>1200914017679814661</td>\n",
       "      <td>Bloody hell Asda @AsdaServiceTeam shopping due...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:06:28.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>6da127116b06cece</td>\n",
       "      <td>Upchurch, England</td>\n",
       "      <td>Upchurch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     selected_tweets                   id  \\\n",
       "0  huge thank you to peer project for allowing me...  1200923972742848512   \n",
       "1  woohoo first new friend since moving to liverp...  1200922864142241793   \n",
       "2  had anxiety about conceiving for couple of rea...  1200918723449606146   \n",
       "3  wouldn even know where to look imo there is fa...  1200914479661359104   \n",
       "4  bloody hell asda shopping due between pm it no...  1200914017679814661   \n",
       "\n",
       "                                                text  hashtags_cnt  \\\n",
       "0  Huge thank you to @Empowerment_ #peer4U projec...             2   \n",
       "1  Woohoo! First new friend since moving to Liver...             0   \n",
       "2  I had anxiety about conceiving for a couple of...             0   \n",
       "3  @JBattye I wouldn’t even know where to look! I...             0   \n",
       "4  Bloody hell Asda @AsdaServiceTeam shopping due...             0   \n",
       "\n",
       "          hashtags                created_at     tw_date          place_id  \\\n",
       "0  peer4U,codesign  2019-11-30T23:46:01.000Z  2019-11-30  13b1318251fcfc7c   \n",
       "1              NaN  2019-11-30T23:41:37.000Z  2019-11-30  151b9e91272233d1   \n",
       "2              NaN  2019-11-30T23:25:10.000Z  2019-11-30  38eabeb176f29378   \n",
       "3              NaN  2019-11-30T23:08:18.000Z  2019-11-30  778909dfad43f3d6   \n",
       "4              NaN  2019-11-30T23:06:28.000Z  2019-11-30  6da127116b06cece   \n",
       "\n",
       "                 full_name           name coordinates coord_type  \n",
       "0      Bilsborrow, England     Bilsborrow         NaN        NaN  \n",
       "1       Liverpool, England      Liverpool         NaN        NaN  \n",
       "2  East Kilbride, Scotland  East Kilbride         NaN        NaN  \n",
       "3    Huddersfield, England   Huddersfield         NaN        NaN  \n",
       "4        Upchurch, England       Upchurch         NaN        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_tweets_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empath \n",
    "\n",
    "## Empath Resources \n",
    "\n",
    "- https://github.com/Ejhfast/empath-client\n",
    "- https://www.tandfonline.com/doi/abs/10.1080/09638237.2020.1739251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import empath\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from empath import Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the lexicon object \n",
    "# list of categories within empath \n",
    "lexicon = Empath()\n",
    "list_cat = ['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical', 'colors', 'id', 'injury and death', 'demo', 'what']\n",
    "\n",
    "pos_cat = [\"worship\", \"masculine\", \"death\", \"weakness\", \"divine\", \"religion\", \"sleep\", \"swearing_terms\", \"injury\", \"envy\"]\n",
    "neg_cat = [\"gain\", \"reading\", \"banking\", \"independence\", \"programming\", \"payment\", \"technology\", \"tourism\", \"air_travel\", \"negotiate\"]\n",
    "tot_cat = pos_cat + neg_cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to analyse sentences \n",
    "# can be altered to add category and normalise values \n",
    "# may need to specifiy categories \n",
    "\n",
    "def empath_analyse(sentence):\n",
    "    x = lexicon.analyze(sentence, categories=tot_cat)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running tweets through empath \n",
    "- Create an array of the 'cleaned' text from the tweets \n",
    "- Run a for loop to analyse tweets usinh emapth_analyse function \n",
    "- Convert data dictionary into dataframe  \n",
    "- Append data dictionary dataframe to the exsiting tweet dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_list = demo_tweets_clean['selected_tweets'].values.tolist()\n",
    "data_to_list = np.array(data_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worship</th>\n",
       "      <th>masculine</th>\n",
       "      <th>death</th>\n",
       "      <th>weakness</th>\n",
       "      <th>divine</th>\n",
       "      <th>religion</th>\n",
       "      <th>sleep</th>\n",
       "      <th>swearing_terms</th>\n",
       "      <th>injury</th>\n",
       "      <th>envy</th>\n",
       "      <th>gain</th>\n",
       "      <th>reading</th>\n",
       "      <th>banking</th>\n",
       "      <th>independence</th>\n",
       "      <th>programming</th>\n",
       "      <th>payment</th>\n",
       "      <th>technology</th>\n",
       "      <th>tourism</th>\n",
       "      <th>air_travel</th>\n",
       "      <th>negotiate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worship  masculine  death  weakness  divine  religion  sleep  \\\n",
       "0      0.0        0.0    0.0       0.0     0.0       0.0    0.0   \n",
       "1      0.0        0.0    0.0       0.0     0.0       0.0    0.0   \n",
       "2      0.0        0.0    0.0       0.0     1.0       0.0    0.0   \n",
       "3      0.0        0.0    0.0       0.0     0.0       0.0    0.0   \n",
       "4      0.0        0.0    0.0       0.0     0.0       0.0    0.0   \n",
       "\n",
       "   swearing_terms  injury  envy  gain  reading  banking  independence  \\\n",
       "0             0.0     1.0   0.0   0.0      1.0      0.0           0.0   \n",
       "1             0.0     0.0   0.0   0.0      0.0      0.0           0.0   \n",
       "2             0.0     0.0   0.0   1.0      0.0      0.0           0.0   \n",
       "3             0.0     0.0   0.0   1.0      1.0      0.0           0.0   \n",
       "4             1.0     1.0   0.0   0.0      0.0      0.0           0.0   \n",
       "\n",
       "   programming  payment  technology  tourism  air_travel  negotiate  \n",
       "0          0.0      0.0         1.0      0.0         0.0        0.0  \n",
       "1          0.0      0.0         0.0      0.0         0.0        0.0  \n",
       "2          0.0      0.0         0.0      0.0         0.0        0.0  \n",
       "3          0.0      0.0         0.0      0.0         0.0        0.0  \n",
       "4          0.0      0.0         0.0      0.0         0.0        0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =[]\n",
    "for i in range(len(data_to_list)):\n",
    "    data.append(empath_analyse(data_to_list[i])),\n",
    "data_dict = pd.DataFrame.from_dict(data)\n",
    "data_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emapth Analysis Complete!\n"
     ]
    }
   ],
   "source": [
    "result = pd.concat([demo_tweets_clean, data_dict], axis=1, join=\"inner\")\n",
    "print(\"Emapth Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_tweets</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags_cnt</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tw_date</th>\n",
       "      <th>place_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>gain</th>\n",
       "      <th>reading</th>\n",
       "      <th>banking</th>\n",
       "      <th>independence</th>\n",
       "      <th>programming</th>\n",
       "      <th>payment</th>\n",
       "      <th>technology</th>\n",
       "      <th>tourism</th>\n",
       "      <th>air_travel</th>\n",
       "      <th>negotiate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huge thank you to peer project for allowing me...</td>\n",
       "      <td>1200923972742848512</td>\n",
       "      <td>Huge thank you to @Empowerment_ #peer4U projec...</td>\n",
       "      <td>2</td>\n",
       "      <td>peer4U,codesign</td>\n",
       "      <td>2019-11-30T23:46:01.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>13b1318251fcfc7c</td>\n",
       "      <td>Bilsborrow, England</td>\n",
       "      <td>Bilsborrow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>woohoo first new friend since moving to liverp...</td>\n",
       "      <td>1200922864142241793</td>\n",
       "      <td>Woohoo! First new friend since moving to Liver...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:41:37.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>151b9e91272233d1</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>had anxiety about conceiving for couple of rea...</td>\n",
       "      <td>1200918723449606146</td>\n",
       "      <td>I had anxiety about conceiving for a couple of...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:25:10.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>38eabeb176f29378</td>\n",
       "      <td>East Kilbride, Scotland</td>\n",
       "      <td>East Kilbride</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wouldn even know where to look imo there is fa...</td>\n",
       "      <td>1200914479661359104</td>\n",
       "      <td>@JBattye I wouldn’t even know where to look! I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:08:18.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>778909dfad43f3d6</td>\n",
       "      <td>Huddersfield, England</td>\n",
       "      <td>Huddersfield</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bloody hell asda shopping due between pm it no...</td>\n",
       "      <td>1200914017679814661</td>\n",
       "      <td>Bloody hell Asda @AsdaServiceTeam shopping due...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-30T23:06:28.000Z</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>6da127116b06cece</td>\n",
       "      <td>Upchurch, England</td>\n",
       "      <td>Upchurch</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     selected_tweets                   id  \\\n",
       "0  huge thank you to peer project for allowing me...  1200923972742848512   \n",
       "1  woohoo first new friend since moving to liverp...  1200922864142241793   \n",
       "2  had anxiety about conceiving for couple of rea...  1200918723449606146   \n",
       "3  wouldn even know where to look imo there is fa...  1200914479661359104   \n",
       "4  bloody hell asda shopping due between pm it no...  1200914017679814661   \n",
       "\n",
       "                                                text  hashtags_cnt  \\\n",
       "0  Huge thank you to @Empowerment_ #peer4U projec...             2   \n",
       "1  Woohoo! First new friend since moving to Liver...             0   \n",
       "2  I had anxiety about conceiving for a couple of...             0   \n",
       "3  @JBattye I wouldn’t even know where to look! I...             0   \n",
       "4  Bloody hell Asda @AsdaServiceTeam shopping due...             0   \n",
       "\n",
       "          hashtags                created_at     tw_date          place_id  \\\n",
       "0  peer4U,codesign  2019-11-30T23:46:01.000Z  2019-11-30  13b1318251fcfc7c   \n",
       "1              NaN  2019-11-30T23:41:37.000Z  2019-11-30  151b9e91272233d1   \n",
       "2              NaN  2019-11-30T23:25:10.000Z  2019-11-30  38eabeb176f29378   \n",
       "3              NaN  2019-11-30T23:08:18.000Z  2019-11-30  778909dfad43f3d6   \n",
       "4              NaN  2019-11-30T23:06:28.000Z  2019-11-30  6da127116b06cece   \n",
       "\n",
       "                 full_name           name  ... gain reading  banking  \\\n",
       "0      Bilsborrow, England     Bilsborrow  ...  0.0     1.0      0.0   \n",
       "1       Liverpool, England      Liverpool  ...  0.0     0.0      0.0   \n",
       "2  East Kilbride, Scotland  East Kilbride  ...  1.0     0.0      0.0   \n",
       "3    Huddersfield, England   Huddersfield  ...  1.0     1.0      0.0   \n",
       "4        Upchurch, England       Upchurch  ...  0.0     0.0      0.0   \n",
       "\n",
       "   independence  programming  payment  technology  tourism  air_travel  \\\n",
       "0           0.0          0.0      0.0         1.0      0.0         0.0   \n",
       "1           0.0          0.0      0.0         0.0      0.0         0.0   \n",
       "2           0.0          0.0      0.0         0.0      0.0         0.0   \n",
       "3           0.0          0.0      0.0         0.0      0.0         0.0   \n",
       "4           0.0          0.0      0.0         0.0      0.0         0.0   \n",
       "\n",
       "   negotiate  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result[\"id\"] = result.index\n",
    "# saving non aggregated dataset of empath analysed tweets \n",
    "result.to_csv('/Users/eleanordavies/Desktop/mean_pivot_table.csv', index = False) \n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_date</th>\n",
       "      <th>count_of_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tw_date  count_of_tweets\n",
       "0  2019-11-27               18\n",
       "1  2019-11-28              199\n",
       "2  2019-11-29              162\n",
       "3  2019-11-30              112\n",
       "4  2019-12-16              835"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting total number of tweets per date \n",
    "\n",
    "pivot_table = result.pivot_table(\n",
    "     index='tw_date',\n",
    "     values='id',\n",
    "     aggfunc= len).reset_index()\n",
    "\n",
    "pivot_table.columns = ['tw_date','count_of_tweets']\n",
    "\n",
    "pivot_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_date</th>\n",
       "      <th>air_travel</th>\n",
       "      <th>banking</th>\n",
       "      <th>death</th>\n",
       "      <th>divine</th>\n",
       "      <th>envy</th>\n",
       "      <th>gain</th>\n",
       "      <th>independence</th>\n",
       "      <th>injury</th>\n",
       "      <th>masculine</th>\n",
       "      <th>...</th>\n",
       "      <th>payment</th>\n",
       "      <th>programming</th>\n",
       "      <th>reading</th>\n",
       "      <th>religion</th>\n",
       "      <th>sleep</th>\n",
       "      <th>swearing_terms</th>\n",
       "      <th>technology</th>\n",
       "      <th>tourism</th>\n",
       "      <th>weakness</th>\n",
       "      <th>worship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.226131</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.025126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.104938</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.067901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.141975</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.049383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.008929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.287425</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.221557</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.131737</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.011976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.359281</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.317365</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.089820</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.011976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.024242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2021-01-12</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.207483</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.066893</td>\n",
       "      <td>0.123583</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.171202</td>\n",
       "      <td>0.045351</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.006803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>0.309392</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.121547</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.027624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.382060</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.049834</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.315615</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106312</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.106312</td>\n",
       "      <td>0.073090</td>\n",
       "      <td>0.086379</td>\n",
       "      <td>0.089701</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>0.039867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tw_date  air_travel   banking     death    divine      envy      gain  \\\n",
       "0   2019-11-27    0.000000  0.111111  0.722222  0.000000  0.000000  0.000000   \n",
       "1   2019-11-28    0.035176  0.030151  0.175879  0.030151  0.030151  0.035176   \n",
       "2   2019-11-29    0.055556  0.104938  0.209877  0.067901  0.000000  0.061728   \n",
       "3   2019-11-30    0.044643  0.053571  0.080357  0.044643  0.008929  0.035714   \n",
       "4   2019-12-16    0.023952  0.035928  0.287425  0.023952  0.035928  0.065868   \n",
       "..         ...         ...       ...       ...       ...       ...       ...   \n",
       "59  2021-01-10    0.005988  0.041916  0.359281  0.023952  0.023952  0.011976   \n",
       "60  2021-01-11    0.012121  0.036364  0.218182  0.054545  0.012121  0.042424   \n",
       "61  2021-01-12    0.031746  0.032880  0.142857  0.020408  0.013605  0.032880   \n",
       "62  2021-01-13    0.033149  0.055249  0.270718  0.044199  0.011050  0.049724   \n",
       "63  2021-01-14    0.000000  0.126246  0.382060  0.036545  0.019934  0.049834   \n",
       "\n",
       "    independence    injury  masculine  ...   payment  programming   reading  \\\n",
       "0       0.000000  0.444444   0.000000  ...  0.277778     0.000000  0.166667   \n",
       "1       0.030151  0.226131   0.050251  ...  0.050251     0.005025  0.105528   \n",
       "2       0.043210  0.197531   0.012346  ...  0.080247     0.061728  0.098765   \n",
       "3       0.026786  0.232143   0.053571  ...  0.062500     0.026786  0.133929   \n",
       "4       0.035928  0.221557   0.035928  ...  0.059880     0.017964  0.131737   \n",
       "..           ...       ...        ...  ...       ...          ...       ...   \n",
       "59      0.023952  0.317365   0.023952  ...  0.035928     0.041916  0.083832   \n",
       "60      0.006061  0.230303   0.036364  ...  0.048485     0.024242  0.096970   \n",
       "61      0.013605  0.207483   0.026077  ...  0.032880     0.066893  0.123583   \n",
       "62      0.027624  0.309392   0.049724  ...  0.060773     0.071823  0.060773   \n",
       "63      0.013289  0.315615   0.033223  ...  0.106312     0.019934  0.106312   \n",
       "\n",
       "    religion     sleep  swearing_terms  technology   tourism  weakness  \\\n",
       "0   0.000000  0.000000        0.111111    0.000000  0.000000  0.000000   \n",
       "1   0.050251  0.140704        0.060302    0.050251  0.025126  0.035176   \n",
       "2   0.037037  0.141975        0.080247    0.030864  0.012346  0.030864   \n",
       "3   0.017857  0.098214        0.071429    0.044643  0.008929  0.053571   \n",
       "4   0.053892  0.155689        0.053892    0.059880  0.011976  0.035928   \n",
       "..       ...       ...             ...         ...       ...       ...   \n",
       "59  0.035928  0.125749        0.035928    0.089820  0.005988  0.047904   \n",
       "60  0.018182  0.133333        0.054545    0.048485  0.030303  0.030303   \n",
       "61  0.006803  0.171202        0.045351    0.112245  0.032880  0.019274   \n",
       "62  0.033149  0.121547        0.038674    0.088398  0.011050  0.033149   \n",
       "63  0.073090  0.086379        0.089701    0.046512  0.019934  0.033223   \n",
       "\n",
       "     worship  \n",
       "0   0.000000  \n",
       "1   0.025126  \n",
       "2   0.049383  \n",
       "3   0.008929  \n",
       "4   0.011976  \n",
       "..       ...  \n",
       "59  0.011976  \n",
       "60  0.024242  \n",
       "61  0.006803  \n",
       "62  0.027624  \n",
       "63  0.039867  \n",
       "\n",
       "[64 rows x 21 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating data by date and get mean of lexical categories \n",
    "\n",
    "mean_pivot_table = result.pivot_table(\n",
    "    index=[ \"tw_date\"], # \"name\"], \n",
    "    values=tot_cat,\n",
    "    aggfunc=np.mean).reset_index()\n",
    "mean_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pivot_table['count'] = mean_pivot_table.tw_date.map(\n",
    "   pivot_table.set_index('tw_date').count_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_date</th>\n",
       "      <th>air_travel</th>\n",
       "      <th>banking</th>\n",
       "      <th>death</th>\n",
       "      <th>divine</th>\n",
       "      <th>envy</th>\n",
       "      <th>gain</th>\n",
       "      <th>independence</th>\n",
       "      <th>injury</th>\n",
       "      <th>masculine</th>\n",
       "      <th>...</th>\n",
       "      <th>programming</th>\n",
       "      <th>reading</th>\n",
       "      <th>religion</th>\n",
       "      <th>sleep</th>\n",
       "      <th>swearing_terms</th>\n",
       "      <th>technology</th>\n",
       "      <th>tourism</th>\n",
       "      <th>weakness</th>\n",
       "      <th>worship</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-28</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.175879</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.226131</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-29</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.104938</td>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.067901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.141975</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.133929</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.098214</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.287425</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.221557</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.131737</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.059880</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.359281</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>0.317365</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.125749</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.089820</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.042424</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.048485</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.024242</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2021-01-12</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.207483</td>\n",
       "      <td>0.026077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066893</td>\n",
       "      <td>0.123583</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.171202</td>\n",
       "      <td>0.045351</td>\n",
       "      <td>0.112245</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.270718</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>0.309392</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.121547</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.382060</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.049834</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.315615</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.106312</td>\n",
       "      <td>0.073090</td>\n",
       "      <td>0.086379</td>\n",
       "      <td>0.089701</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.019934</td>\n",
       "      <td>0.033223</td>\n",
       "      <td>0.039867</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tw_date  air_travel   banking     death    divine      envy      gain  \\\n",
       "0   2019-11-27    0.000000  0.111111  0.722222  0.000000  0.000000  0.000000   \n",
       "1   2019-11-28    0.035176  0.030151  0.175879  0.030151  0.030151  0.035176   \n",
       "2   2019-11-29    0.055556  0.104938  0.209877  0.067901  0.000000  0.061728   \n",
       "3   2019-11-30    0.044643  0.053571  0.080357  0.044643  0.008929  0.035714   \n",
       "4   2019-12-16    0.023952  0.035928  0.287425  0.023952  0.035928  0.065868   \n",
       "..         ...         ...       ...       ...       ...       ...       ...   \n",
       "59  2021-01-10    0.005988  0.041916  0.359281  0.023952  0.023952  0.011976   \n",
       "60  2021-01-11    0.012121  0.036364  0.218182  0.054545  0.012121  0.042424   \n",
       "61  2021-01-12    0.031746  0.032880  0.142857  0.020408  0.013605  0.032880   \n",
       "62  2021-01-13    0.033149  0.055249  0.270718  0.044199  0.011050  0.049724   \n",
       "63  2021-01-14    0.000000  0.126246  0.382060  0.036545  0.019934  0.049834   \n",
       "\n",
       "    independence    injury  masculine  ...  programming   reading  religion  \\\n",
       "0       0.000000  0.444444   0.000000  ...     0.000000  0.166667  0.000000   \n",
       "1       0.030151  0.226131   0.050251  ...     0.005025  0.105528  0.050251   \n",
       "2       0.043210  0.197531   0.012346  ...     0.061728  0.098765  0.037037   \n",
       "3       0.026786  0.232143   0.053571  ...     0.026786  0.133929  0.017857   \n",
       "4       0.035928  0.221557   0.035928  ...     0.017964  0.131737  0.053892   \n",
       "..           ...       ...        ...  ...          ...       ...       ...   \n",
       "59      0.023952  0.317365   0.023952  ...     0.041916  0.083832  0.035928   \n",
       "60      0.006061  0.230303   0.036364  ...     0.024242  0.096970  0.018182   \n",
       "61      0.013605  0.207483   0.026077  ...     0.066893  0.123583  0.006803   \n",
       "62      0.027624  0.309392   0.049724  ...     0.071823  0.060773  0.033149   \n",
       "63      0.013289  0.315615   0.033223  ...     0.019934  0.106312  0.073090   \n",
       "\n",
       "       sleep  swearing_terms  technology   tourism  weakness   worship  count  \n",
       "0   0.000000        0.111111    0.000000  0.000000  0.000000  0.000000     18  \n",
       "1   0.140704        0.060302    0.050251  0.025126  0.035176  0.025126    199  \n",
       "2   0.141975        0.080247    0.030864  0.012346  0.030864  0.049383    162  \n",
       "3   0.098214        0.071429    0.044643  0.008929  0.053571  0.008929    112  \n",
       "4   0.155689        0.053892    0.059880  0.011976  0.035928  0.011976    835  \n",
       "..       ...             ...         ...       ...       ...       ...    ...  \n",
       "59  0.125749        0.035928    0.089820  0.005988  0.047904  0.011976    835  \n",
       "60  0.133333        0.054545    0.048485  0.030303  0.030303  0.024242    825  \n",
       "61  0.171202        0.045351    0.112245  0.032880  0.019274  0.006803    882  \n",
       "62  0.121547        0.038674    0.088398  0.011050  0.033149  0.027624   1086  \n",
       "63  0.086379        0.089701    0.046512  0.019934  0.033223  0.039867    903  \n",
       "\n",
       "[64 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pivot_table.to_csv('/Users/eleanordavies/Desktop/mean_pivot_table.csv', index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
