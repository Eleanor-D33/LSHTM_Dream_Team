{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Access "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API access resources\n",
    "\n",
    "- https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "- https://docs.tweepy.org/en/latest/\n",
    "- https://docs.tweepy.org/en/latest/api.html#search-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing package 'Tweepy' \n",
    "\n",
    "pip install -- tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access token for twitter API \n",
    "# appname <- \"LSHTM_DC_2021\"\n",
    "# access keys and secret removed for public display use \n",
    "\n",
    "consumer_key = \" \"\n",
    "consumer_secret = \" \"\n",
    "\n",
    "access_token = \" \"\n",
    "access_token_secret = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenicating connection \n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter general search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"lockdown\" + \" -filter:retweets\" + \" - filter:media\"\n",
    "result_type = \"mixed\"\n",
    "date_since = \"2021-01-21\"\n",
    "until = \"2021-01-26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cursor to access the API \n",
    "# searching tweets \n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=date_since, \n",
    "              until = until,\n",
    "              result_type = result_type,\n",
    "                  tweet_mode=\"extended\").items(500)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# picking the columns to display \n",
    "# display as a dataframe \n",
    "users_locs = [[tweet.id, tweet.full_text, tweet.user.screen_name, tweet.user.location, tweet.created_at] for tweet in tweets]\n",
    "tweet_df = pd.DataFrame(data=users_locs, columns=['tweet_id','text', 'user', \"location\", \"date\"])\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.to_csv('/Users/eleanordavies/Desktop/tweet_lockdown_demo.csv', index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter timeline search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = \"BorisJohnson\"\n",
    "result_type = \"mixed\"\n",
    "date_since = \"2018-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bojo = tw.Cursor(api.user_timeline,\n",
    "              screen_name= user_name,\n",
    "              lang=\"en\",\n",
    "              since_id=date_since, \n",
    "              result_type = result_type,\n",
    "              exclude_replies = True,\n",
    "              include_rts = False,\n",
    "                  tweet_mode=\"extended\").items(5000)\n",
    "bojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_timeline_locs = [[tweet.id, tweet.full_text, tweet.user.screen_name, tweet.user.location, tweet.created_at] for tweet in bojo]\n",
    "bojo_df = pd.DataFrame(data=users_timeline_locs, columns=['tweet_id','text', 'user', \"location\", \"date\"])\n",
    "bojo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources used for pre-processing \n",
    "\n",
    "- https://towardsdatascience.com/an-easy-tutorial-about-sentiment-analysis-with-deep-learning-and-keras-2bf52b9cba91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove URLs from the tweets\n",
    "* Tokenize text\n",
    "* Remove emails\n",
    "* Remove new lines characters\n",
    "* Remove distracting single quotes\n",
    "* Remove all punctuation signs\n",
    "* Lowercase all text\n",
    "* Detokenize text\n",
    "* Convert list of texts to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "tweet_text = pd.read_csv('/Users/eleanordavies/Desktop/tweet_lockdown_demo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to remove characters \n",
    "\n",
    "def depure_data(data):\n",
    "    \n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "#Splitting twtter data text column to list\n",
    "data_to_list = tweet_text['text'].values.tolist()\n",
    "\n",
    "#applying the above function \n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "\n",
    "tempdf = pd.DataFrame(temp)\n",
    "tempdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize removes all punctation, emojis and puts text into lowercase \n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "data_words = list(sent_to_words(temp))\n",
    "\n",
    "print(data_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_detoken = []\n",
    "for i in range(len(data_words)):\n",
    "    data_detoken.append(detokenize(data_words[i]))\n",
    "data_detoken = np.array(data_detoken)\n",
    "data_detoken_df = pd.DataFrame(data_detoken)\n",
    "data_detoken_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_detoken_df.to_csv('/Users/eleanordavies/Desktop/df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [data_detoken_df,tweet_text]\n",
    "demo_tweets_clean = pd.concat(frames, axis=1)\n",
    "\n",
    "demo_tweets_clean = demo_tweets_clean.rename(columns={0: \"selected_tweets\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_tweets_clean.to_csv('/Users/eleanordavies/Desktop/demo_tweets_clean.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
